{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) and was written by Erwan Lecarpentier and Jonathan Sprauel.\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">XGBoost<br>Introduction to XGBoost</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[XGBoost](https://xgboost.readthedocs.io/en/latest/) is a library. It implements machine learning algorithms (Figure 1) that are all working with the [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) framework. It can be used for regression and classification. It produces efficient models to deal with standard tabular data as opposed to more fancy data structures like images, sounds, videos etc.\n",
    "\n",
    "<img id=\"fig1\" src=\"img/machine_learning.png\">\n",
    "<center>Figure 1: a machine learning algorithm</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Practice Course is composed of 3 parts - each part is meant to be done in about 1 hour :\n",
    "* In the **first notebook**, you will learn the **basic of XGBoost**, how to apply it on a dataset and tune it to obtain the best performances.\n",
    "* In the **second notebook**, we will focus on **ensemble methods** and explain what makes XGBoost different from other models.\n",
    "* Finally in the **last notebook** you will see how the choice of a method (such as XGBoost) is a key element of a tradeoff between **Bias and Variance**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a id=\"sec1\"></a> What is XGBoost?\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is a library for Gradient Tree Boosting in C++, Java, Python, R and Julia, that was initially developped by Tianqi Chen.\n",
    "XGBoost has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. For reference (and inspiration), you can have a look at this curated list of first, second and third place competition [winners that used XGBoost](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions).\n",
    "\n",
    "> When in doubt, use xgboost.\n",
    "> [Avito Winnerâ€™s Interview](http://blog.kaggle.com/2015/08/26/avito-winners-interview-1st-place-owen-zhang/)\n",
    "\n",
    "\n",
    "XGBoost has been designed for speed and performance : it is usually faster than sklearn's GradientBoost, even though they are fundamentally the same as they are both gradient boosting implementations.\n",
    "\n",
    "## Installing XGBoost\n",
    "\n",
    "With Anaconda use :\n",
    "```conda install -c anaconda py-xgboost```\n",
    "\n",
    "With pip use :\n",
    "```pip install xgboost```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install xgboost -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple XGBoost usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, be sure that you have the following dependencies and run the import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn numpy pandas sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost natively uses a specific format, that is optimized in term of memory and computation speed : DMatrix.\n",
    "When using numpy, you have to convert it explicitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major parameters are as following - we will focus on the parameters later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 3,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # Learning rate, the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quieter and faster\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 3}  # the number of classes that exist in this dataset\n",
    "num_round = 20  # the number of training iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the training and prediction with an api that is similar to sklearn - though we have to the true prediction : for each line, we must select the class where the probability is the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "preds = bst.predict(dtest)\n",
    "predictions = np.asarray([np.argmax(line) for line in preds])\n",
    "#print (precision_score(y_test, predictions, average='macro')) # 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a bit less performance, you can also directly use the pandas-compatible functions that mirror exactly the sklearn api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "mod = XGBClassifier(**param)\n",
    "mod.fit(X_train, y_train)\n",
    "predictions = mod.predict(X_test)\n",
    "#print (precision_score(y_test, predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can save the model either with pickle or with the dedicated function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain, num_round)\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "bst.save_model('xgb.model')\n",
    "\n",
    "bst2 = xgb.Booster(model_file='xgb.model')\n",
    "\n",
    "preds2 = bst2.predict(dtest)\n",
    "# assert they are the same\n",
    "assert np.sum(np.abs(preds2 - preds)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A complete exemple : Classification of stars, Galaxies, Quasars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first application of XGBoost, we will try to classify observations of space to be either stars, galaxies or quasars.\n",
    "We are using data from the [Sloan Digital Sky Survey](http://www.sdss.org/)\n",
    "\n",
    "### About the SDSS\n",
    "The Sloan Digital Sky Survey is a project which offers public data of space observations. Observations have been made since 1998 and have been made accessible to everyone who is interested.\n",
    "\n",
    "For this purpose a special 2.5 m diameter telescope was built at the Apache Point Observatory in New Mexico, USA. The telescope uses a camera of 30 CCD-Chips with 2048x2048 image points each. The chips are ordered in 5 rows with 6 chips in each row. Each row observes the space through different optical filters (u, g, r, i, z) at wavelengths of approximately 354, 476, 628, 769, 925 nm.\n",
    "\n",
    "The telescope covers around one quarter of the earth's sky - therefore focuses on the northern part of the sky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_df = pd.read_csv('Skyserver_SQL2_27_2018 6_51_39 PM.csv', skiprows=1)\n",
    "sdss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cells, we only checked a few classical elements when facing a dataset : \n",
    "* There is no missing data, that we should complete\n",
    "* Most features remain within reasonable values, for each columns\n",
    "\n",
    "The goal is to classify each data into either the Galaxy, Star or QSO class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Before applying any classification algorithm, let's look a bit more and transform the data : first we remove the column that obviously won't help classify into the correct class, such as the objects id and parameters of the camera at the moment of observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_df.drop(['objid', 'run', 'rerun', 'camcol', 'field', 'specobjid'], axis=1, inplace=True)\n",
    "sdss_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at a few interesting features (univariate analysis) : by plotting the distribution of each class along this feature, we can estimate if this feature can help in classifying the data.\n",
    "\n",
    "For instance, we can see that redshift seems to have good correlation, while ascension and declination does not differ significantly between the 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3,figsize=(16, 4))\n",
    "ax = sns.distplot(sdss_df[sdss_df['class']=='STAR'].redshift, bins = 30, ax = axes[0], kde = False)\n",
    "ax.set_title('Star')\n",
    "ax = sns.distplot(sdss_df[sdss_df['class']=='GALAXY'].redshift, bins = 30, ax = axes[1], kde = False)\n",
    "ax.set_title('Galaxy')\n",
    "ax = sns.distplot(sdss_df[sdss_df['class']=='QSO'].redshift, bins = 30, ax = axes[2], kde = False)\n",
    "ax = ax.set_title('QSO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='ra', y='dec', data=sdss_df, hue='class', fit_reg=False, palette='coolwarm', height=6, aspect=2)\n",
    "plt.title('Equatorial coordinates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transform a few features : we transform the different bands through a PCA, we encode the classes and scale the extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_df_fe = sdss_df\n",
    "\n",
    "# encode class labels to integers\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(sdss_df_fe['class'])\n",
    "sdss_df_fe['class'] = y_encoded\n",
    "\n",
    "# Principal Component Analysis\n",
    "pca = PCA(n_components=3)\n",
    "ugriz = pca.fit_transform(sdss_df_fe[['u', 'g', 'r', 'i', 'z']])\n",
    "\n",
    "# update dataframe \n",
    "sdss_df_fe = pd.concat((sdss_df_fe, pd.DataFrame(ugriz)), axis=1)\n",
    "sdss_df_fe.rename({0: 'PCA_1', 1: 'PCA_2', 2: 'PCA_3'}, axis=1, inplace = True)\n",
    "sdss_df_fe.drop(['u', 'g', 'r', 'i', 'z'], axis=1, inplace=True)\n",
    "sdss_df_fe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "sdss = scaler.fit_transform(sdss_df_fe.drop('class', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Using XGboost is similar in many ways with the sklearn api : we define a model and call the *fit* function on the training data; the *predict* function makes the prediction.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Exercice 1 :</b> Complete the following functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(..., test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgbC = XGBClassifier(n_estimators=100)\n",
    "xgbC.fit(...)\n",
    "preds = xgbC.predict(...)\n",
    "acc_xgb = (preds == y_test).sum().astype(float) / len(preds)*100\n",
    "print(\"XGBoost's prediction accuracy is: %3.2f\" % (acc_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "...\n",
    "acc_rfc = (preds == y_test).sum().astype(float) / len(preds)*100\n",
    "print(\"Scikit-Learn's Random Forest Classifier's prediction accuracy is: %3.2f\" % (acc_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "svc = SVC()\n",
    "...\n",
    "acc_svc = (preds == y_test).sum().astype(float) / len(preds)*100\n",
    "print(\"Scikit-Learn's Support Vector Machine Classifier's prediction accuracy is: %3.2f\" % (acc_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can focus a bit more on the performance comparison between XGBoost and Random Forest, in term of optimality :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc_cv = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_val_score(rfc_cv, X_train, y_train, cv=10, scoring = \"accuracy\")\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Mean:\", scores.mean())\n",
    "print(\"Standard Deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv = XGBClassifier(n_estimators=100)\n",
    "scores = cross_val_score(xgb_cv, X_train, y_train, cv=10, scoring = \"accuracy\")\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Mean:\", scores.mean())\n",
    "print(\"Standard Deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, XGBoost also allows to obtain the feature importance list :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({\n",
    "    'Feature': sdss_df_fe.drop('class', axis=1).columns,\n",
    "    'Importance': xgbC.feature_importances_\n",
    "})\n",
    "importances = importances.sort_values(by='Importance', ascending=False)\n",
    "importances = importances.set_index('Feature')\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A focus on the parameters\n",
    "\n",
    "XGBoost has <[many parameters](https://xgboost.readthedocs.io/en/latest/parameter.html); we will explain them a bit by following this guideline, that woks well on most problems :\n",
    "\n",
    "* Choose a relatively high learning rate (**learning_rate**)\n",
    "* Determine the optimum number of trees for this learning rate. (**n_estimators**)\n",
    "* Tune tree-specific parameters ( **max_depth, min_child_weight, gamma, subsample, colsample_bytree** for instance) for the decided learning rate and number of trees.\n",
    "* Tune regularization parameters (**lambda, alpha**)\n",
    "\n",
    "For most of the functions, instead of relying on sklearn cross_valisation function we will use XGBoost function called \"cv\" which performs cross-validation at each boosting iteration and returns the optimum number of trees required.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Exercice 2 :</b> Complete the following functions.\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 100\n",
    "param = {\n",
    "    \"learning_rate\" :0.3,\n",
    "    \"n_estimators\":1000,\n",
    "    'silent': 1,\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3\n",
    "}\n",
    "\n",
    "...\n",
    "\n",
    "cvresult = xgb.cv(param, dtrain, nfold=10, num_boost_round=param['n_estimators'],  early_stopping_rounds=50)\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "preds = bst.predict(dtest)\n",
    "predictions = np.asarray([np.argmax(line) for line in preds])\n",
    "print (precision_score(y_test, predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvresult.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Exercice 3 :</b> Complete the following code to tune the tree parameters : max_depth and min_child_weight\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {\n",
    "    \"learning_rate\" :0.3,\n",
    "    \"n_estimators\":...,\n",
    "    \"max_depth\":7,\n",
    "    \"min_child_weight\":2,\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3\n",
    "}\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':...,\n",
    " 'min_child_weight':...\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(param), param_grid = param_test1, cv=10)\n",
    "\n",
    "gsearch1.fit(X_train, y_train)\n",
    "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process can be repeated iteratively with the remaining parameters. A good heuristic is to follow this order next :\n",
    "Gamma, then (subsample and colsample_bytree) together, then reg_lambda and reg_alpha.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Exercice 3 :</b> Complete the tuning and try to obtain the best performance of the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# replace parameters for final evaluation\n",
    "param = {\n",
    "    \"learning_rate\" :0.1,\n",
    "    \"n_estimators\":76,\n",
    "    \"max_depth\":6,\n",
    "    \"min_child_weight\":1,\n",
    "    \"gamma\":0,\n",
    "    \"subsample\":0.8,\n",
    "    \"colsample_bytree\":0.8,\n",
    "    \"nthread\":4,\n",
    "    \"scale_pos_weight\":1,\n",
    "    'silent': 1,\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3\n",
    "}\n",
    "\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "preds = bst.predict(dtest)\n",
    "predictions = np.asarray([np.argmax(line) for line in preds])\n",
    "print (precision_score(y_test, predictions, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion of the notebook\n",
    "\n",
    "In the notebook, we have seen the basic functions to use XGBoost. \n",
    "The next two notebooks will be more open-ended : the first one will be focused on the ensemble methods, while the second one will be focused on the tuning of the parameters, with regards to the tradeoff between biais and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources : \n",
    "* https://www.kaggle.com/lucidlenn/data-analysis-and-classification-using-xgboost\n",
    "* https://github.com/dmlc/xgboost/tree/master/demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
