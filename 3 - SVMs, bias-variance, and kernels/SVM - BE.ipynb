{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "209863ed-cab4-4427-9861-05e0e4b27167"
    }
   },
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Support Vector Machines,<br> the Bias-Variance tradeoff<br> and a short introduction to kernel theory</div>\n",
    "\n",
    "In the previous notebook, we saw the basic of SVM, its formulation as a quadratic optimization problem and the introduction of the regularization term C.\n",
    "\n",
    "In this notebook, we will further study this parameter, move on to the notion of *overfit* and *underfit* and introduce kernels.\n",
    "\n",
    "2. [Support Vector Machines in scikit-learn](#sec2)\n",
    "3. [When using linear separators makes no more sense](#sec3)\n",
    "4. [A word on the bias-variance tradeoff](#sec4)\n",
    "5. [The kernel trick](#sec5)\n",
    "    1. [The intuition of mapping to higher dimensions](#sec5-1)\n",
    "    2. [The kernel trick](#sec5-2)\n",
    "    3. [Positive definite kernels](#sec5-3)\n",
    "6. [SVMs and kernels](#sec6)\n",
    "7. [What about other uses?](#sec7)\n",
    "8. [Examples](#sec8)\n",
    "    1. [Spam or ham?](#sec8-1)\n",
    "    2. [NIST](#sec8-2)\n",
    "7. [Multi-Label Classification](#sec9)\n",
    "\n",
    "<div class=\"alert alert-success\"><b>In a nutshell:</b>\n",
    "<ul>\n",
    "<li> Support Vector Machines try to separate data by maximizing a geometrical margin\n",
    "<li> They are computed offline\n",
    "<li> They offer a sparse, robust to class imbalance, and easy to evaluate predictor\n",
    "<li> Kernels are a way of enriching (lifting) the data representation so that it becomes linearly separable\n",
    "<li> SVMs + kernels offer a versatile method for classification, regression and density estimation\n",
    "<li> [Documentation in scikit-learn](http://scikit-learn.org/stable/modules/svm.html)\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "575227d2-0b4e-43fd-a4f3-c32f6163a9ed"
    }
   },
   "source": [
    "# 2. <a id=\"sec2\"></a>Support Vector Machines in scikit-learn\n",
    "\n",
    "Let's start by loading the dataset of the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "fig_size=(10,10)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "res = np.loadtxt(\"sep_lin.csv\", delimiter=',')\n",
    "X = res[:,0:-1]\n",
    "y = res[:,-1].astype(int)\n",
    "Xblue = X[y==-1]\n",
    "Xred = X[y==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately for us, this optimization problem is solved is three lines in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6dce34d9-ee79-4aaa-86c1-274cce55dec2"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "mySVC = svm.SVC(kernel='linear', C=0.1)\n",
    "mySVC.fit(X,y)\n",
    "\n",
    "# Compute margin and find support vectors\n",
    "w = mySVC.coef_[0]\n",
    "w0 = mySVC.intercept_\n",
    "M = 1./np.linalg.norm(w)\n",
    "print(\"SV per class:\", mySVC.n_support_)\n",
    "print(\"w_i:\", w)\n",
    "print(\"w_0:\", w0)\n",
    "print(\"Margin:\", M)\n",
    "print(\"w^T x0 + w_0:\", np.dot(w,mySVC.support_vectors_[0,:])+w0)\n",
    "print(\"w^T x1 + w_0:\", np.dot(w,mySVC.support_vectors_[1,:])+w0)\n",
    "print(\"w^T x2 + w_0:\", np.dot(w,mySVC.support_vectors_[2,:])+w0)\n",
    "\n",
    "# Plot the separating plane, the margin and the Support Vectors\n",
    "fig=plt.figure(figsize=fig_size, dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(Xblue[:,0],Xblue[:,1],c='b',s=20)\n",
    "plt.scatter(Xred[:,0],Xred[:,1],c='r',s=20)\n",
    "XX = np.arange(-1.,12.,0.1)\n",
    "YY = -(w[0]*XX+w0)/w[1]\n",
    "plt.plot(XX,YY,'g')\n",
    "YY = -(w[0]*XX+w0+M)/w[1]\n",
    "plt.plot(XX,YY,'g--')\n",
    "YY = -(w[0]*XX+w0-M)/w[1]\n",
    "plt.plot(XX,YY,'g--')\n",
    "plt.scatter(mySVC.support_vectors_[:,0], mySVC.support_vectors_[:,1], s=80, edgecolors='k', facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "23cb012c-0452-4012-8595-eeb54e756485"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice**<br>\n",
    "Try different values for $C$ in the code above to see how the margin's boundaries evolve.\n",
    "</div>\n",
    "\n",
    "Remark: since the optimization problem behind SVMs requires the knowledge of the full dataset, SVMs are necessarily an offline method.\n",
    "\n",
    "Let's plot how the number of support vectors change with $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e5ee8e83-c4af-4aba-a915-8282132cc5ac"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Recall, there are\", Xblue.shape[0], \"blue points and\", Xred.shape[0], \"red points.\")\n",
    "logC = np.arange(-4,7,1)\n",
    "nbSV = np.zeros(len(logC))\n",
    "for i in range(len(logC)):\n",
    "    print(\"Training at C =\", 10.**logC[i])\n",
    "    mySVC = svm.SVC(kernel='linear', C=10.**logC[i])\n",
    "    mySVC.fit(X,y)\n",
    "    nbSV[i] = np.sum(mySVC.n_support_)\n",
    "\n",
    "fig=plt.figure(figsize=fig_size, dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(logC,nbSV)\n",
    "plt.xlabel(\"log C\")\n",
    "plt.ylabel(\"nb support vectors\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7fc6c322-c6f0-461a-a5dd-9cf34eb1ba13"
    }
   },
   "source": [
    "Let's make a few predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "92964068-7386-49e4-8fe9-796899ee0576"
    }
   },
   "outputs": [],
   "source": [
    "mySVC = svm.SVC(kernel='linear', C=1)\n",
    "mySVC.fit(X,y)\n",
    "\n",
    "# Compute margin and find support vectors\n",
    "w = mySVC.coef_[0]\n",
    "w0 = mySVC.intercept_\n",
    "M = 1./np.linalg.norm(w)\n",
    "print(\"SV per class:\", mySVC.n_support_)\n",
    "print(\"w_i:\", w)\n",
    "print(\"w_0:\", w0)\n",
    "print(\"Margin:\", M)\n",
    "print(\"w^T x0 + w_0:\", np.dot(w,mySVC.support_vectors_[0,:])+w0)\n",
    "print(\"w^T x1 + w_0:\", np.dot(w,mySVC.support_vectors_[1,:])+w0)\n",
    "print(\"w^T x2 + w_0:\", np.dot(w,mySVC.support_vectors_[2,:])+w0)\n",
    "\n",
    "# Plot the separating plane, the margin and the Support Vectors\n",
    "fig=plt.figure(figsize=fig_size, dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(Xblue[:,0],Xblue[:,1],c='b',s=20)\n",
    "plt.scatter(Xred[:,0],Xred[:,1],c='r',s=20)\n",
    "XX = np.arange(-1.,12.,0.1)\n",
    "YY = -(w[0]*XX+w0)/w[1]\n",
    "plt.plot(XX,YY,'g')\n",
    "YY = -(w[0]*XX+w0+M)/w[1]\n",
    "plt.plot(XX,YY,'g--')\n",
    "YY = -(w[0]*XX+w0-M)/w[1]\n",
    "plt.plot(XX,YY,'g--')\n",
    "\n",
    "# Testing data\n",
    "Xtest = np.array([[2,4],[6,2],[10,2]])\n",
    "Ypred = mySVC.predict(Xtest)\n",
    "for i in range(Xtest.shape[0]):\n",
    "    print(\"Prediction in\", Xtest[i,:], \"=\", Ypred[i])\n",
    "\n",
    "plt.scatter(Xtest[:,0],Xtest[:,1], c='m', edgecolors=\"k\", s=80);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "83029f02-43d2-428d-b957-2fe7ceceffab"
    }
   },
   "source": [
    "# <a id=\"sec3\"></a>3. When using linear separators makes no more sense\n",
    "\n",
    "Let's take a look at the following data. Would it make sense to try to separate the red points from the blue ones with a straight line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "33109de0-9ac6-4138-8574-b26f067ac171"
    }
   },
   "outputs": [],
   "source": [
    "X1, y1 = datasets.make_gaussian_quantiles(cov=2.,\n",
    "                                 n_samples=300, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X2, y2 = datasets.make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                 n_samples=700, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, - y2 + 1))\n",
    "y = 2*y-1\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "Xtest,X = np.split(X,[400])\n",
    "ytest,y = np.split(y,[400])\n",
    "\n",
    "Xblue = X[y==-1]\n",
    "Xred = X[y==1]\n",
    "fig=plt.figure(figsize=fig_size, dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(Xblue[:,0],Xblue[:,1],c='b')\n",
    "plt.scatter(Xred[:,0],Xred[:,1],c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "973f28e4-dc48-47bf-b401-f1c1d14938c5"
    }
   },
   "source": [
    "It does not look like a linear separation even makes sense. But let's try nonetheless, some optimal separating hyperplane must exist (even if it does not have great generalization properties).\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Use scikit-learn to compute a linear Support Vector Classifier for the data above. Compute the classification score and conclude on the limits of this approach.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d08cd339-291e-43cb-8689-0e83ee74843f"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/code3.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d8994a10-ee7b-47a2-892e-8b195de373bb"
    }
   },
   "source": [
    "Quite unconvincing, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A short metaphor before we go into more details**\n",
    "\n",
    "<img src=\"img/crumpled-paper.jpg\" width=\"400px\"></img>\n",
    "\n",
    "Take two blank sheets of paper. On the first one, draw a set of blue points. On the second, draw some red points. Now put one page on top of the other and crumple them together. The red and blue points are separable (whatever number of points there were on each sheet) , it's a certainty, since you put them on purpose on two different pieces of paper (two classes). But if you're given the current coordinates of each point, then building a relevant classifier might not be so easy. Some ideas follow from this intuition:\n",
    "- Good classification might be feasible if we could build an inverse transformation (\"uncrumpling\") of the data that makes it linearly separable.\n",
    "- Good classification functions should be able to approximate a large variety of geometric transformations of the data (this inverse transformation should be able to capture a large variety of crumpling patterns).\n",
    "- A classification method that can only approximate a few geometrical transformations is prone to miss the structure of the data. We call that *underfitting*, which is due to *high bias* in the family of classification functions.\n",
    "- Having a large set of uncrumpling patterns (being able to approximate many geometrical transformations of the data) comes at the price of taking the risk of picking the wrong uncrumpling pattern given the finite amount of data at our disposal. We call that *overfitting*, which is due to *high variance* in the family of classification functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "42fadc80-3a4c-4a4c-9425-043e104c62f8"
    }
   },
   "source": [
    "# <a id=\"sec5\"></a>5. The kernel trick\n",
    "\n",
    "## <a id=\"sec5-1\"></a> 5.1 The intuition of mapping to higher dimensions\n",
    "\n",
    "Let's get back to the idea of giving more flexibility to our separator, since hyperplanes did not convince us they could always do the job.\n",
    "\n",
    "To get around this problem, let's first introduce an illustrative example. The data below come from a voltage test in electronics. They indicate if a component fails or not under a certain voltage ($U$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f25a5d30-b13b-4532-85bd-f61b89f8315e"
    }
   },
   "outputs": [],
   "source": [
    "U = np.array([[0.3, 0.7, 1.1, 1.8, 2.5, 3.0, 3.3, 3.5, 3.7]]).T\n",
    "Good = np.array( [[ -1,  -1,  -1,   1,   1,   1,   1,  -1,  -1]]).T\n",
    "plt.figure()\n",
    "plt.scatter(U, np.zeros((U.shape[0],1)), c=Good, cmap = plt.cm.coolwarm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "217e5e28-6b8d-4e79-9147-cdbb61f790ab"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question:**<br>Does it look like these data are linearly separable?\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Some well-experienced engineer already knows that $U$ is no good criterion to split the data points into two categories with a single threshold (recall: in dimension 1, a hyperplane is a threshold). He also knows that other indicators like $V = (U-2)^2$ are not any better, but that the pair $(U,V)$ actually allows to build a separating line between points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7547c3d6-ccd0-49cd-97a9-a92ccf7b20b2"
    }
   },
   "outputs": [],
   "source": [
    "V = (2-U)**2\n",
    "plt.figure()\n",
    "plt.scatter(U, V, c=Good, cmap = plt.cm.coolwarm)\n",
    "XX = np.arange(np.min(U), np.max(U), 0.05)\n",
    "YY = -.8+.8*XX\n",
    "plt.plot(XX,YY,'g');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7ad8607f-7316-49b0-a25c-38924256edaf"
    }
   },
   "source": [
    "Our nine points are now linearly separable. But we have not introduced any additional information!<br>\n",
    "<br>\n",
    "Actually, the only thing we have done, is to *map* our non-separable points $x$ into **another description space**, of higher dimension, where a linear separator exists.<br>\n",
    "\n",
    "## <a id=\"sec5-2\"></a> 5.2 The kernel trick\n",
    "\n",
    "Suppose we happen to know such a relevant mapping for our data $\\varphi(x)=x'$, where $\\varphi$ maps to a $p$-dimensional Euclidean space ($p\\gg n$, possibly infinite). Then we can compute the optimal linear separator in the corresponding higher dimension description space, find its parameters $w'$ and $w'_0$, and whenever we need to make a new prediction in a point $x$, we first compute its image $x'=\\varphi(x)$ and then calculate $w'^T x' + w'_0$ to know on which side of the hyperplane we stand.\n",
    "\n",
    "But recall that: $$w' = \\sum_{i=1}^N \\alpha_i y_i x'_i$$\n",
    "\n",
    "So :\n",
    "\\begin{align*}\n",
    "w'^T x' + w'_0 & = \\left(\\sum_{i=1}^N \\alpha_i y_i x'_i\\right)^T \\varphi(x) + w_0\\\\\n",
    "& = \\sum_{i=1}^N \\alpha_i y_i \\varphi(x_i)^T \\varphi(x) + w'_0\n",
    "\\end{align*}\n",
    "\n",
    "Suppose that, instead of providing us with a mapping $\\varphi(x)=x'$, somebody gave us a function $K(x_1, x_2)$ that takes two points $x_1$ and $x_2$, computes their respective images $\\varphi(x_1)$ and $\\varphi(x_2)$ and returns the dot product:\n",
    "$$K(x_1,x_2)=\\varphi(x_1)^T \\varphi(x_2)$$\n",
    "\n",
    "Then making predictions would boil down to:\n",
    "$$w'^T x' + w'_0 = \\sum_{i=1}^N \\alpha_i y_i K(x_i,x) + w'_0$$\n",
    "\n",
    "The interesting thing is that we don't need to compute $\\varphi$ anymore. The function $K$ is known as a **kernel function** and that's what we call the **kernel trick**.\n",
    "\n",
    "It is actually possible to compute the $\\alpha_i$ and $w'_0$ just using $K$ and never $\\varphi$, so, as long as somebody insures that **the kernel $K$ is a dot product in some other descriptor space** we can compute the optimal separating hyperplane of our data in this space, **without ever requiring a knowledge of this descriptor space and the mapping $\\varphi$**.\n",
    "\n",
    "## <a id=\"sec5-3\"></a> 5.3 Positive definite kernels\n",
    "\n",
    "The main question is thus: when can we guarantee that $K$ is an acceptable kernel? It is so if it is an inner product on a (separable) Hilbert space. So in more general words, we are interested in positive, definite kernels on a Hilbert space:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "$K(\\cdot,\\cdot)$ is a positive definite kernel on $X$ if\n",
    "$$\\forall n\\in\\mathbb{N}, x\\in X^n \\text{ and } c\\in \\mathbb{R}^n, \\ \\sum\\limits_{i,j=1}^n c_i c_j K(x_i,x_j) \\geq 0$$\n",
    "</div>\n",
    "\n",
    "We will admit that Mercer's condition below is a sufficient condition for $K$ to be a positive definite kernel:\n",
    "<div class=\"alert alert-success\">\n",
    "Given $K(x,y)$, if:\n",
    "$$\\forall g(x) / \\int g(x)^2dx <\\infty, \\iint K(x,y)g(x)g(y)dxdy \\geq 0$$\n",
    "Then, there exists a mapping $h(\\cdot)$ such that:\n",
    "$$K(x,y) = \\langle h(x), h(y) \\rangle$$\n",
    "</div>\n",
    "\n",
    "There are many kernels that have been developped in the litterature. Combined with what we have seen before, it allows to build non-linear SVMs. The nice thing is that some kernels actually map our data to a descriptor space of infinite dimension, where it is presumably a lot easier to find a separating hyperplane.\n",
    "\n",
    "One such kernel is the so-called \"radial basis kernel\" which is very popular and can be written:\n",
    "$$K(x_1,x_2) = e^{-\\gamma \\|x_1-x_2\\|^2}$$\n",
    "\n",
    "Other common kernels:\n",
    "- polynomial $K(x,y)=\\left(1+\\langle x, y\\rangle\\right)^d$\n",
    "- sigmoid $K(x,y) = \\tanh\\left(\\kappa_1 \\langle x, y\\rangle + \\kappa_2\\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bf8918ea-6de2-4759-a168-32990eb1de38"
    }
   },
   "source": [
    "# <a id=\"sec6\"></a>6. SVMs and kernels\n",
    "\n",
    "Let's practice on the last dataset with a radial basis kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fa7dbe63-50b7-4b7b-881b-fc1128e18dcc"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot separator, margin and support vectors\n",
    "def plot_SVC(mySVC):\n",
    "    fig=plt.figure(figsize=fig_size, dpi= 80, facecolor='w', edgecolor='k')\n",
    "    plt.scatter(Xblue[:,0],Xblue[:,1],c='b')\n",
    "    plt.scatter(Xred[:,0],Xred[:,1],c='r')\n",
    "    XX, YY = np.meshgrid(np.arange(np.min(X[:,0]),np.max(X[:,0]),0.1), np.arange(np.min(X[:,1]),np.max(X[:,1]),0.1))\n",
    "    ZZ = mySVC.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "    ZZ = ZZ.reshape(XX.shape)\n",
    "    plt.contour(XX, YY, ZZ, levels=[0],alpha=0.75)\n",
    "    fig=plt.figure(figsize=fig_size, dpi= 80, facecolor='w', edgecolor='k')\n",
    "    cont = plt.contour(XX, YY, ZZ, levels=[-1., 0., 1.], alpha=0.75)\n",
    "    plt.clabel(cont, cont.levels, inline=True, fontsize=10)\n",
    "    fig=plt.figure(figsize=fig_size, dpi= 80, facecolor='w', edgecolor='k')\n",
    "    cont = plt.contourf(XX, YY, ZZ, alpha=0.75, cmap = plt.cm.coolwarm)\n",
    "\n",
    "# We can play with C\n",
    "mySVC = svm.SVC(kernel='rbf', C=1.)\n",
    "mySVC.fit(X,y)\n",
    "\n",
    "print(\"SV per class:\", mySVC.n_support_)\n",
    "plot_SVC(mySVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "71301663-2487-40a8-87c6-74a4d1183162"
    }
   },
   "source": [
    "A lot better isn't it? Let's play a little with the value of $C$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Let's see how that SVM with the rbf kernel would do on the data from the beginning of this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e4dc7f1d-2d84-46bb-aa86-6571b3b7b9e9"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load solutions/code4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "92ff468b-8cd1-4b92-acbb-b13a3e540fee"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "And on the \"linearly separable + noise\" case?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "21e389bc-335d-4b93-aca8-0637b709b2df"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X1, y1 = datasets.make_gaussian_quantiles(cov=2.0, n_samples=300, n_features=2, n_classes=1)\n",
    "X1[:,0] = 3. + X1[:,0]\n",
    "X1[:,1] = 6. + X1[:,1]/2.5\n",
    "X2, y2 = datasets.make_gaussian_quantiles(cov=1.5, n_samples=300, n_features=2, n_classes=1)\n",
    "X2[:,0] = 8. + X2[:,0]\n",
    "X2[:,1] = 4. + X2[:,1]\n",
    "X3, y3 = datasets.make_gaussian_quantiles(cov=1.5, n_samples=300, n_features=2, n_classes=1)\n",
    "X3[:,0] = 7. + X3[:,0]\n",
    "X3[:,1] = 8. + X3[:,1]\n",
    "X = np.concatenate((X1, X2, X3))\n",
    "y = np.concatenate((y1, - y2 + 1, y3))\n",
    "y = 2*y-1\n",
    "X, y = shuffle(X, y)\n",
    "Xblue = X[y==-1]\n",
    "Xred = X[y==1]\n",
    "\n",
    "# Display\n",
    "mySVC = svm.SVC(kernel='rbf')\n",
    "mySVC.fit(X,y)\n",
    "print(\"SV per class:\", mySVC.n_support_)\n",
    "plot_SVC(mySVC);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cc280e3d-b852-4b4b-89a1-238b30cf1441"
    }
   },
   "source": [
    "These two last experiments illustrate that introducing complex kernels such as the rbf one is sometimes detrimental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "109dd2fd-c49e-44a9-8a48-911877fce53e"
    }
   },
   "source": [
    "# <a id=\"sec8\"></a>8. Examples\n",
    "\n",
    "## <a id=\"sec8-1\"></a>8.1 Spam or ham?\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Your turn to play: let's build a spam classifier using the ling-spam dataset and a linear SVM. Compute it's score on the validation dataset below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "67bbc140-344b-4510-8635-72df3e783e70"
    }
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('../2 - Text data preprocessing')\n",
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtrain.shape)\n",
    "print(Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySVM = svm.SVC(kernel='rbf')\n",
    "mySVM.fit(Xtrain, ytrain)\n",
    "#plotting\n",
    "plot_SVC(mySVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9e414752-9438-46d7-b036-e2af3e670979"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/code5.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "We've trained our model in the Tf-Idf data. Let's see how the model behaves on raw word counts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/code6.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Want to try another kernel than linear?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the tf-idf with linear SVM classifier (which seems to work better) and use it to identify which are the misclassified emails (and find the confusion matrix by the way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "873f04f7-2a40-45f9-9a03-970f71cd5928"
    }
   },
   "outputs": [],
   "source": [
    "# Retrain\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)\n",
    "spam_svc = svm.SVC(kernel='linear', C=1.)\n",
    "spam_svc.fit(Xtrain,ytrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bab94b21-0f19-4ab5-81c6-7601ced11400"
    }
   },
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "ypredict = spam_svc.predict(Xtest)\n",
    "misclass = np.not_equal(ypredict, ytest)\n",
    "Xmisclass = Xtest[misclass,:]\n",
    "ymisclass = ytest[misclass]\n",
    "misclass_indices = [i for i, j in enumerate(misclass) if j == True]\n",
    "print(\"Misclassified messages indices:\", misclass_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, ypredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some misclassified mails\n",
    "index = misclass_indices[0]+2000\n",
    "print(\"Prediction:\", spam_svc.predict(spam_data.tfidf[index,:]))\n",
    "spam_data.print_email(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "710e351f-00b6-41bc-aff8-a7a203435248"
    }
   },
   "source": [
    "## <a id=\"sec8-2\"></a>8.2. NIST\n",
    "\n",
    "Let's evaluate SVMs on the optical character recognition task of the NIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b5946f3a-cce1-4138-af17-0fb2cb788f38"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "print(digits.data.shape)\n",
    "print(digits.images.shape)\n",
    "print(digits.target.shape)\n",
    "print(digits.target_names)\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "Xtrain,Xtest = np.split(X,[1000])\n",
    "ytrain,ytest = np.split(y,[1000])\n",
    "#Xtrain = X[:1000,:]\n",
    "#ytrain = y[:1000]\n",
    "#Xtest = X[1000:,:]\n",
    "#ytest = y[1000:]\n",
    "\n",
    "print(digits.DESCR)\n",
    "\n",
    "plt.gray();\n",
    "plt.matshow(digits.images[0]);\n",
    "plt.show();\n",
    "plt.matshow(digits.images[15]);\n",
    "plt.show();\n",
    "plt.matshow(digits.images[42]);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "428a56b4-f981-4118-8ce6-9147576f53e4"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_and_split(X,y,n):\n",
    "    X0,y0 = shuffle(X,y)\n",
    "    Xtrain,Xtest = np.split(X0,[n])\n",
    "    ytrain,ytest = np.split(y0,[n])\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7aee1580-4a5f-461d-9bdf-9632ae1da643"
    }
   },
   "outputs": [],
   "source": [
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "digits_svc = svm.SVC(kernel='rbf', gamma=1e-3)\n",
    "digits_svc.fit(Xtrain,ytrain)\n",
    "prediction = digits_svc.predict(Xtest)\n",
    "#print(\"Training error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest))\n",
    "print(\"Generalization error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest) )\n",
    "print(\"Generalization score:\", digits_svc.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "55d61d0e-45a0-4442-9a27-6ac040e0c599"
    }
   },
   "outputs": [],
   "source": [
    "# Compute cross-validation score\n",
    "nb_trials = 20\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = shuffle_and_split(X,y,1000)\n",
    "    digits_svc = svm.SVC(kernel='rbf', gamma=1e-3)\n",
    "    digits_svc.fit(Xtrain,ytrain)\n",
    "    score += [digits_svc.score(Xtest,ytest)]\n",
    "    print('*',end='')\n",
    "print(\" done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b9942737-31dd-44c7-a08f-95f7a75a83f5"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify which are the misclassified images (and find the confusion matrix by the way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "51a0fedf-7c31-4527-a389-32d247fa89be"
    }
   },
   "outputs": [],
   "source": [
    "# Retrain\n",
    "Xtrain = X[:1000,:]\n",
    "ytrain = y[:1000]\n",
    "Xtest = X[1000:,:]\n",
    "ytest = y[1000:]\n",
    "digits_svc = svm.SVC(kernel='rbf', gamma=1e-3)\n",
    "digits_svc.fit(Xtrain,ytrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c4d2ac07-1cf7-465f-9089-1b132459f980"
    }
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "N = 1053\n",
    "plt.matshow(digits.images[N]) \n",
    "plt.show() \n",
    "x = digits.data[N,:]\n",
    "print(\"prediction on image number\", N, \":\", digits_svc.predict([digits.data[N,:]]))\n",
    "print(\"correct label                :\", digits.target[N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c7c0ae00-4061-4d28-a9da-419c4a11f21a"
    }
   },
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "ypredict = digits_svc.predict(Xtest)\n",
    "misclass = np.not_equal(ypredict, ytest)\n",
    "Itest = digits.images[1000:,:]\n",
    "Xmisclass = Xtest[misclass,:]\n",
    "ymisclass = ytest[misclass]\n",
    "Imisclass = Itest[misclass,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, ypredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4f03a16d-a638-444e-881f-cbb26eac5685"
    }
   },
   "outputs": [],
   "source": [
    "# Display misclassified examples\n",
    "N = 1\n",
    "plt.matshow(Imisclass[N]) \n",
    "plt.show() \n",
    "print(\"prediction on image number\", N, \":\", digits_svc.predict([Xmisclass[N,:]]))\n",
    "print(\"correct label                :\", ymisclass[N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f04d09ee-0e3c-4a24-b3ce-345323c1ce23"
    }
   },
   "source": [
    "# 9. <a id=\"sec9\"></a> Application: Multi-label Classification (MLC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see an application which is both harder and less common than binary classification, that of **multi-label classification** (MLC).  \n",
    "Given a list of possible labels, the problem consists in finding one **or more** labels associated to a data point.  \n",
    "For instance, imagine extracting the keywords from a newspaper article. Possibly many labels can be associated to a text; or for example classifing the elements composing an image\n",
    "\n",
    "Given a set of labels $\\mathcal{L} = \\{l_1, l_2, ..., l_k\\} \\in \\{0,1\\}^k$, we want to map elements of a feature space $\\mathcal{X}$ to a subset of $\\mathcal{L}$:  \n",
    "\n",
    "$$h : \\mathcal{X} \\longrightarrow \\mathcal{P}(\\mathcal{L})$$\n",
    "\n",
    "The two typical approaches for such problems are known as **Binary Relevance** (BR) and **Label Powerset** (LP).  \n",
    "\n",
    " - BR: each label in $\\mathcal{L}$ is a binary classification problem, $h_{i} : \\mathcal{X} \\longrightarrow l_{i}, l_{i} \\in \\{0,1\\}, i = 1, ..., |\\mathcal{L}|$.\n",
    "\n",
    " - LP: transforms a problem of MLC into one of multiclass classification, mapping elements $x \\in \\mathcal{X}$ directly to $s \\in \\mathcal{P}(\\mathcal{L})$.  \n",
    " This method becomes rapidly inapplicable as the number of $s$ grows exponentially.\n",
    " \n",
    " Many other variations exist, but for today we'll focus on BR, the most straightforward to implement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use a biology dataset from [Elisseeff and Weston 2001]: this dataset contains micro-array expressions and phylogenetic profiles for 2417 yeast genes. Each gen is annotated with a subset of 14 functional categories (e.g. Metabolism, energy, etc.) of the top level of the functional catalogue.\n",
    "\n",
    "<div class=\"alert alert-warning\">**Exercice**<br>\n",
    "<ul>\n",
    "\n",
    "<li> find a suitable package to load the file at `yeast.arff`.  <br>\n",
    "    Hint: <a href=https://docs.scipy.org/doc/scipy/reference/io.html>scipy.io</a> and _read the doc_.<br>\n",
    "<li> Store the data in a pandas dataframe.<br>\n",
    "    Hint: columns of classes will be encoded as 'utf-8', we need integers, look for 'str.decode('utf-8')'\n",
    "<li> check dataset: you should have 2417 samples $\\times$ 117 columns (103 features + 14 labels)\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code8.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">**Exercice**<br>\n",
    "<ul>\n",
    "<li> Manually, fit a SVM classifier for each label in the dataset\n",
    "<li> Apply a cross-validation of 60 ∕ 40: 60% of datapoints to train the model, 40% to test it  <br>\n",
    "   Remember: it is good practice to **randomly shuffle** the data, in case the data are ordered w.r.t. some data-dependent criterion.\n",
    "<li> Report some performance measure\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code9.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulation**, you reached the end of the BE ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
